# -*- coding: utf-8 -*-
"""diceloss_cell.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dPah42uQ7l70LrJyEPJbiJAU27riXA5r
"""

!nvidia-smi

import os
os.mkdir('/content/stage1_train')

# !cp -r '/content/drive/MyDrive/ĐA2/data_cell/stage1_train.zip' '/content/stage1_train'

import shutil
shutil.copyfile('/content/drive/MyDrive/ĐA2/data_cell/stage1_train.zip', '/content/stage1_train.zip')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/stage1_train/
!unzip /content/stage1_train.zip

# from pydrive.auth import GoogleAuth
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials
# from zipfile import ZipFile

# os.mkdir('/content/train')
# folder_name = '/content/train'
# with ZipFile('/content/stage1_train/stage1_train.zip') as f:
#   f.extractall(folder_name)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import RMSprop
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import xml.etree.ElementTree as ET
import sklearn
import itertools
import cv2
import scipy
import os
import csv
import matplotlib.pyplot as plt
# %matplotlib inline

import glob
import random
import cv2
import matplotlib.pyplot as plt

# image_files = []
# image_masks = []
# train_folder = '/content/train'
# image_folder = glob.glob(train_folder+'/*')
# for i in range(len(image_folder)):
#   image_files.append(glob.glob(image_folder[i] + '/images/*'))
#   image_masks.append(glob.glob(image_folder[i] + '/masks/*'))

# class DataGenerator(keras.utils.Sequence):
# 	'Generates data for Keras'
# 	def __init__(self, images_paths, labels, batch_size=64, image_dimensions = (96 ,96 ,3), shuffle=False, augment=False):
# 		self.labels       = labels              # array of labels
# 		self.images_paths = images_paths        # array of image paths
# 		self.dim          = image_dimensions    # image dimensions
# 		self.batch_size   = batch_size          # batch size
# 		self.shuffle      = shuffle             # shuffle bool
# 		self.augment      = augment             # augment data bool
# 		self.on_epoch_end()

# 	def __len__(self):
# 		'Denotes the number of batches per epoch'
# 		return int(np.floor(len(self.images_paths) / self.batch_size))

# 	def on_epoch_end(self):
# 		'Updates indexes after each epoch'
# 		self.indexes = np.arange(len(self.images_paths))
# 		if self.shuffle:
# 			np.random.shuffle(self.indexes)

# 	def __getitem__(self, index):
# 		'Generate one batch of data'
# 		# selects indices of data for next batch
# 		indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]

# 		# select data and load images
# 		labels = np.array([self.labels[k] for k in indexes])
# 		images = [cv2.imread(self.images_paths[k]) for k in indexes]
        
# 		# preprocess and augment data
# 		# if self.augment == True:
# 		#     images = self.augmentor(images)
		
# 		images = np.array([preprocess_input(img) for img in images])
# 		return images, labels

from keras.utils import Sequence
class DataGenerator(Sequence):
  def __init__(self, folder, ids, batch_size=64, image_size = (256 ,256), shuffle=False, augment=False):
    self.folder = folder
    # ids = next(os.walk(folder))[1]
    img_dirs = [os.path.join(self.folder, aa, 'images') for aa in ids]
    self.image_paths = [os.path.join(aa, next(os.walk(aa))[2][0]) for aa in img_dirs]
    self.mask_dirs = [os.path.join(self.folder, aa, 'masks') for aa in ids]
    self.image_size          = image_size    # image dimensions
    self.batch_size   = batch_size          # batch size
    self.shuffle      = shuffle             # shuffle bool
    self.augment      = augment             # augment data bool
    self.on_epoch_end()
    print('data length:', len(self.image_paths))
  def __len__(self):
    return int(np.floor(len(self.image_paths) / self.batch_size))
  def on_epoch_end(self):
    self.indexes = np.arange(len(self.image_paths))
    if self.shuffle:
      np.random.shuffle(self.indexes)
  def __getitem__(self, index):
    # selects indices of data for next batch
    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]
    # select data and load images
    # labels = np.array([self.labels[k] for k in indexes])
    labels = []
    for k in indexes:
      mask_dir = self.mask_dirs[k]
      mask_names = next(os.walk(mask_dir))[2]
      mask_paths = [os.path.join(mask_dir, aa) for aa in mask_names]
      masks = np.array([cv2.imread(aa)[...,0]/255. for aa in mask_paths], dtype = np.float32)
      masks = np.sum(masks, axis=0)
      masks = np.where(masks, 1.0, 0.0)
      masks = cv2.resize(masks, self.image_size)
      labels.append(masks)
    labels = np.array(labels, dtype=np.float32)
    images = np.array([cv2.resize(cv2.imread(self.image_paths[k]), self.image_size) for k in indexes])    
		# preprocess and augment data
		# if self.augment == True:
		#     images = self.augmentor(images)
    # images = np.array([preprocess_input(img) for img in images])
    return images, labels

import numpy as np
from random import shuffle

folder = '/content/stage1_train'
ids = next(os.walk(folder))[1]
len_test = int(0.2*len(ids))
# rand_id = np.random.permutation(len(ids))
shuffle(ids)
ids_test = ids[:len_test]
ids_train = ids[len_test:]

len(ids_test), len((ids_train))

train_gen = DataGenerator(folder, ids_train, 8)
test_gen = DataGenerator(folder, ids_test, 8)

x_te

for img, mask in train_gen:
  # plt.figure(i)
  fig, axs = plt.subplots(1,2, figsize=(12,12))
  axs[0].imshow(img[0], cmap='gray'), axs[0].axis('off')
  axs[1].imshow(mask[0], cmap='gray'), axs[1].axis('off')

from keras.losses import binary_crossentropy
import keras.backend as K
smooth=1.0

def dice_coef(y_true, y_pred):
    y_truef=K.flatten(y_true)
    y_predf=K.flatten(y_pred)
    And=K.sum(y_truef* y_predf)
    return((2* And + smooth) / (K.sum(y_truef) + K.sum(y_predf) + smooth))

def dice_coef_loss(y_true, y_pred):
    return 1.0 - dice_coef(y_true, y_pred)

def iou(y_true, y_pred):
    intersection = K.sum(y_true * y_pred)
    sum_ = K.sum(y_true + y_pred)
    jac = (intersection + smooth) / (sum_ - intersection + smooth)
    return jac

def jac_distance(y_true, y_pred):
    y_truef=K.flatten(y_true)
    y_predf=K.flatten(y_pred)

    return 1.0 - iou(y_true, y_pred)


def bce_dice_loss(y_true, y_pred):
    # return 0.5*binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)
    return 1*binary_crossentropy(y_true, y_pred)+1 - dice_coef(y_true, y_pred)


def BCE_loss(y_true, y_pred):
    # return 0.5*binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)
#       lambda_ac=0.5/(im_width*im_height)
#       lambda_BCE=0.5
      return binary_crossentropy(y_true, y_pred)

from keras.layers import Input, concatenate, Conv2DTranspose
from keras.models import Model

def unet(input_size=(256,256,3)):
    inputs = Input(input_size)
    
    conv1 = Conv2D(64, (3, 3), padding='same')(inputs)
    bn1 = Activation('relu')(conv1)
    conv1 = Conv2D(64, (3, 3), padding='same')(bn1)
    bn1 = BatchNormalization(axis=3)(conv1)
    bn1 = Activation('relu')(bn1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)

    conv2 = Conv2D(128, (3, 3), padding='same')(pool1)
    bn2 = Activation('relu')(conv2)
    conv2 = Conv2D(128, (3, 3), padding='same')(bn2)
    bn2 = BatchNormalization(axis=3)(conv2)
    bn2 = Activation('relu')(bn2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)

    conv3 = Conv2D(256, (3, 3), padding='same')(pool2)
    bn3 = Activation('relu')(conv3)
    conv3 = Conv2D(256, (3, 3), padding='same')(bn3)
    bn3 = BatchNormalization(axis=3)(conv3)
    bn3 = Activation('relu')(bn3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)

    conv4 = Conv2D(512, (3, 3), padding='same')(pool3)
    bn4 = Activation('relu')(conv4)
    conv4 = Conv2D(512, (3, 3), padding='same')(bn4)
    bn4 = BatchNormalization(axis=3)(conv4)
    bn4 = Activation('relu')(bn4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(bn4)

    conv5 = Conv2D(1024, (3, 3), padding='same')(pool4)
    bn5 = Activation('relu')(conv5)
    conv5 = Conv2D(1024, (3, 3), padding='same')(bn5)
    bn5 = BatchNormalization(axis=3)(conv5)
    bn5 = Activation('relu')(bn5)

    up6 = concatenate([Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(bn5), conv4], axis=3)
    conv6 = Conv2D(512, (3, 3), padding='same')(up6)
    bn6 = Activation('relu')(conv6)
    conv6 = Conv2D(512, (3, 3), padding='same')(bn6)
    bn6 = BatchNormalization(axis=3)(conv6)
    bn6 = Activation('relu')(bn6)

    up7 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(bn6), conv3], axis=3)
    conv7 = Conv2D(256, (3, 3), padding='same')(up7)
    bn7 = Activation('relu')(conv7)
    conv7 = Conv2D(256, (3, 3), padding='same')(bn7)
    bn7 = BatchNormalization(axis=3)(conv7)
    bn7 = Activation('relu')(bn7)

    up8 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(bn7), conv2], axis=3)
    conv8 = Conv2D(128, (3, 3), padding='same')(up8)
    bn8 = Activation('relu')(conv8)
    conv8 = Conv2D(128, (3, 3), padding='same')(bn8)
    bn8 = BatchNormalization(axis=3)(conv8)
    bn8 = Activation('relu')(bn8)

    up9 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(bn8), conv1], axis=3)
    conv9 = Conv2D(64, (3, 3), padding='same')(up9)
    bn9 = Activation('relu')(conv9)
    conv9 = Conv2D(64, (3, 3), padding='same')(bn9)
    bn9 = BatchNormalization(axis=3)(conv9)
    bn9 = Activation('relu')(bn9)

    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(bn9)

    return Model(inputs=[inputs], outputs=[conv10])

model = unet((256, 256, 3))

model.summary()

from keras.optimizers import Adam, SGD

opt1 = Adam(lr=3e-4)
loss_func = dice_coef_loss

model.compile(opt1, loss_func, metrics=[dice_coef, jac_distance, iou])

history = model.fit_generator(train_gen, epochs=40, validation_data=test_gen)

import matplotlib.pyplot as plt

plt.plot(history.history['loss'], '-b')
plt.plot(history.history['val_loss'], '-r')
plt.plot(history.history['dice_coef'], '-g')
plt.show()

model.save_weights('/content/drive/MyDrive/ĐA2/cell.h5')

model.load_weights('/content/drive/MyDrive/ĐA2/cell.h5')

predict = model.predict_generator(test_gen)





for img, mask in test_gen:
    prd = model.predict(img)
    fig, axs = plt.subplots(1,2, figsize=(12,12))
    axs[0].imshow(prd[0,...,0], cmap='gray'), axs[0].axis('off')
    axs[1].imshow(mask[0], cmap='gray'), axs[1].axis('off')

for img, mask in test_gen:
  prd = model.predict(img)
  fig, axs = plt.subplots(1,2, figsize=(12,12))
  axs[0].imshow(prd[0,...,0], cmap='gray'), axs[0].axis('off')
  axs[1].imshow(mask[0], cmap='gray'), axs[1].axis('off')

model.evaluate_generator(test_gen)

model.predict(test_gen)

model.predict_generator(test_gen)

# def plot_file_mat(history):
    #   k=history['loss'].shape[1]
    #   plt.figure(1)
    #   plt.plot(np.arange(0,k),history['loss'].reshape(k))
    #   plt.plot(np.arange(0,k),history['dice_coef'].reshape(k))

    #   plt.ylabel('loss/dice_coef')
    #   plt.xlabel('epoch')
    #   plt.legend(['loss', 'dice_coef'], loc=' right')
    #   plt.show()



